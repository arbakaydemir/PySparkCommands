# -*- coding: utf-8 -*-
"""Third Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7KxsaW5u5LtMwjPBXN7rIeyVVydt1dW

# Install and Import necessary Libraries and Pyspark
"""

# Install PySpark in Google Colab
!pip install pyspark

#Import the pySpark
import pyspark

#Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

#Creating a Spark Session
spark = SparkSession.builder.appName("Practise_PySpark").getOrCreate()

#Mounting google drive to access dataset
from google.colab import drive

drive.mount('/content/gdrive')

"""## Load a JSON File"""

#Loading the dataset
file_path = "/content/gdrive/MyDrive/Colab Notebooks/Third Project/dataset.json"

# Read the json file into a DataFrame
df = spark.read.option("multiline","true").json(file_path)

# Display the DataFrame
df.show()

display(df)

"""# DataFrame Operations

Drop a Column
"""

df_new = df.drop("density_per_square_km")
df_new.show()

"""Change a column name"""

df_1 = df.withColumnRenamed("country", "Country")
df_1.show()

"""Change multiple column names"""

df_2 = df.withColumnRenamed("country", "Country").withColumnRenamed(
    "density_per_square_km", "density_squarekm"
)
df_2.show()

"""pyspark.sql.DataFrame.withColumnsRenamed"""

df_3 = df.withColumnsRenamed(
    {"country": "Country", "density_per_square_km": "density_squarekm"}
)
df_3.show()

"""# pyspark.sql.DataFrame.selectðŸ¨"""

df.select("country").show()
#This is useful when you'd like to select a single column

df.select('*').show()

df.select(["country"]).show()
#This is useful when you'd like to select a multiple columns

df.select(["country", "density_per_square_km"]).show()
#Useful when you have a dynamic list of columns.

#Can be easily modified programmatically.

df.select("country", "density_per_square_km").show()
#The columns are specified as multiple string arguments.

#Convenient for hardcoding a fixed set of columns.

#Slightly more concise for a small number of columns.

"""Below code adds 1000 to each row in density_per_square_km column"""

df.select(df["country"],df["density_per_square_km"]+1000).show()

df.select(df["country"],(df["population"] > 100000000).alias("is_population_greater_than_100M")).show()

df.select(col("country"), (col("population") > 100000000).alias("is_population_greater_than_100M")).show()

df.printSchema()

"""## Data Cleaning Conditions;

**In case of dot**:
It is advisible to change datatype to double or float in case we have data such as '2.5' as string. We don't need to remove dots if we want to convert them to a numerical data type. The dot represents a decimal point, and retaining it is essential for preserving the fractional part of the number.

**In case of comma**: Let's say we have comma in the data, and they intend to represent decimal points. In those conditions, we must convert commas to dots, and then we can cast the strings to a nmumerical data type such as float or double.


**In case of percentage**: First we need to remove the percentage sign and then convert the remaining string to a numerical data type, such as float. After removing, we should divide the resulting number by 100 to convert it to a decimal. Check below example:

`# Remove percentage sign and cast to float`

`df = df.withColumn("value", regexp_replace(col("value"), " %", "").cast("float") / 100`

**In case of negative number**: In this condition, we don't need to remove any character. We can directly convert it to an integer.

**In case of negative number contains comma:**
1. Comma as Thousands Separator:
If the comma is intended to separate thousands, you can remove the comma and then cast the string to an integer or float.

`# Remove commas and cast to integer`

`df = df.withColumn("value", regexp_replace(col("value"), ",", "").cast("integer"))`

2. Comma as Decimal Separator:
If the comma is meant to be a decimal point, you should replace the comma with a dot before casting it to a float or double.

`# Replace commas with dots and cast to float`

`df = df.withColumn("value", regexp_replace(col("value"), ",", ".").cast("float"))`

## Data Cleaning Conditions Part 2;
**In case of plus sign with commas:**

Remove the plus sign: The plus sign typically indicates that the value is greater than or equal to the specified number.

Remove commas: If the commas are used as thousands separators.

Cast to an appropriate numerical type: Depending on your need, you can cast it to an integer or float.

`# Remove plus sign, remove commas, and cast to integer`
`df = df.withColumn("value", regexp_replace(col("value"), "[,+]", "").cast("integer"))`

**Using different methods to do same job**

`# Remove plus sign, remove commas, and cast to integer`
`df = df.withColumn("value", regexp_replace(col("value"), "[,+]", "").cast("integer"))`


Alternatively, we can choose to use

`df = df.withColumn("value", regexp_replace(col("value"), "[^0-9]", "").cast("integer"))`

[^0-9]:

This regex pattern matches any character that is not a digit (0-9).

The regexp_replace function will remove all non-digit characters from the "value" column.

**Differences and Similarities:**

**Differences:**

Scope of Removal: The first snippet only removes commas and plus signs, whereas the second snippet removes all non-digit characters. This means the second snippet is more general and can handle various types of non-digit characters.

Use Case: The first snippet is suitable for cases where you specifically want to remove commas and plus signs. The second snippet is suitable for cases where you want to remove any non-numeric characters, ensuring that only digits remain.

**Similarities:**

Outcome: For the specific examples given (like "1,000,000+"), both snippets will produce the same cleaned value ("1000000").

Casting: Both snippets cast the cleaned value to an integer using the cast("integer") method.

**When to Use Each Snippet:**

Use Code Snippet 1:

When your data specifically contains commas and plus signs that you want to remove.

Example: "1,000,000+" to "1000000".

Use Code Snippet 2:

When your data may contain various non-digit characters, and you want to ensure that only numeric digits remain.

Example: "1,000,000+ USD" or "3.5%" to "1000000" and "35", respectively.

## Decision of integer, float and double

**Precision:**

Float: A float (single-precision) has a precision of about 7 decimal digits. It occupies 4 bytes (32 bits) of memory.

Double: A double (double-precision) has a precision of about 15-16 decimal digits. It occupies 8 bytes (64 bits) of memory.

**Range:**

Float: Can represent a smaller range of values compared to double. Itâ€™s typically used for numerical data that does not require a high degree of precision.

Double: Can represent a larger range of values and is used when precision is more critical.

**Memory Usage:**

Float: Requires less memory (4 bytes) and is faster to process.

Double: Requires more memory (8 bytes) but offers greater precision and range.

**When to Use Float:**
Performance: If memory usage and performance are critical (e.g., large datasets or real-time processing) and the precision of 7 decimal digits is sufficient.

Scientific Calculations: Often used in scientific calculations where a rough estimate is acceptable.

**When to Use Double:**
High Precision: When dealing with financial data, scientific calculations, or any application where precise decimal representation is important.

Large Range: When the range of values is significant, such as in large numerical datasets.

**When to use Integer:**
Precision: Exact numerical values without fractional parts.

Use Case: When your data consists of whole numbers.

Example: Population counts, number of items, etc.

## Data Cleaning Necessities:

We need to do some cleaning for below issues.

Let's list them

*   Commas in "density per square km", "land are in square km", "migrants_net", "net change", "population"
*   Percentage sign in "urban population", "world share" and "yearly change" columns.
"""

df.show()

# Let's start with first cleaning necessity. Commas in "density per square km", "land are in square km", "migrants_net", "net change", "population"

df_new1 = df.withColumn("density_per_square_km", regexp_replace(col("density_per_square_km"), ",", "").cast("integer"))\
          .withColumn("land_are_in_square_km", regexp_replace(col("land_are_in_square_km"), ",", "").cast("integer"))\
          .withColumn("migrants_net", regexp_replace(col("migrants_net"), ",", "").cast("integer"))\
          .withColumn("net_change", regexp_replace(col("net_change"), ",", "").cast("integer"))\
          .withColumn("population", regexp_replace(col("population"), ",", "").cast("integer"))\
          .withColumn("fertility_rate", col("fertility_rate").cast("float"))

df_new1.show()
df_new1.printSchema()

"""First necessity is completed succesfully with the above code lines. Now it is time to complete second necessity.

Percentage sign in "urban population", "world share" and "yearly change" columns. We don't need to remove dots.
"""

#Let's start to clean percentage signs, and then we will be converting dataType to the integer in urban population as it doesn't have any decimal point. Then we will convert world_share and yearly change columns into float. Lastly, we need to convert those values into its correct decimal representation to reflect its proper numberical value. For example, "18.47%" should be converted to 0.1847 to represent 18.47 percent as a decimal.
df_new1 = df_new1.withColumn("urban_population", round(regexp_replace(col("urban_population"), "%", "").cast("float") / 100, 5))\
          .withColumn("world_share", round(regexp_replace(col("world_share"), "%", "").cast("float") / 100, 7))\
          .withColumn("yearly_change", round(regexp_replace(col("yearly_change"), "%", "").cast("float") / 100, 7))

df_new1.show()
df_new1.printSchema()

"""## Changing Data Type of a Single Column:"""

df_new1 = df_new1.withColumn("position", col("position").cast("integer"))

df_new1.show(1)

"""Changing Data Type of Multiple Columns:"""

df_new1 = df_new1.withColumn("median_age", col("median_age").cast("integer"))

df_new1.printSchema()

df_new1.select(df_new1["country"],(df_new1["population"] > 100000000).alias("is_population_greater_than_100M")).show()

df_new1.select(col("country"), (col("population") > 100000000).alias("is_population_greater_than_100M")).show()

from pyspark.sql import functions as F

"""## When and Otherwise"""

df_new1.select(
    col("country"),
    F.when(col("population") > 100000000, col("population"))
    .otherwise("Less than 100M")
    .alias("is_population_greater_than_100M")
    ).show()

"""## WHEN AND FILTER TOGETHER"""

df_filtered = df_new1.filter(col("population") > 100000000).select(
    col("country"),
    col("population").alias("population_greater_than_100M")
)

df_filtered.show()

"""## Startswith - Endswith"""

df_startwith = df_new1.filter(col("country").startswith("A"))
df_startwith.show()

df_startwith2 = df_new1.select(df_new1.country.startswith("A"))
df_startwith2.show()

df_filtered = df_new1.filter(col("population") > 100000000)
df_filtered.show()

df_filtered = df_new1.where(col("population") > 100000000)
df_filtered.show()

"""## Get the size of a DataFrame"""

print("{} rows".format(df_new1.count()))
print("{} columns".format(len(df_new1.columns)))

"""## Get a DataFrame's number of partitions:

Performance Tuning: Understanding the number of partitions can help you optimize the performance of your Spark jobs. Adjusting the number of partitions can lead to better parallelism and resource utilization.
"""

print("{} partition(s)".format(df_new1.rdd.getNumPartitions()))

"""What is an RDD?

Resilient Distributed Dataset (RDD): RDDs are the fundamental data structure of Apache Spark. They represent an immutable, distributed collection of objects that can be processed in parallel.

Key Features of RDDs:

Resilient: Fault-tolerant with the ability to recompute missing or damaged partitions due to node failures.

Distributed: Data is distributed across multiple nodes in a cluster, allowing parallel processing.

Dataset: A collection of data elements.

What is a Partition?

Partition: A partition is a logical division of data in an RDD. Each partition is a chunk of data that can be processed independently by a task in Spark.

Key Points:

Partitions enable parallelism: Multiple partitions can be processed simultaneously on different nodes in the cluster.

The number of partitions can impact performance: More partitions can lead to better load balancing and resource utilization, while fewer partitions might reduce the overhead of managing partitions.

Example:
Imagine you have a large dataset of a billion rows. Instead of processing the entire dataset as a single unit, Spark divides it into smaller partitions, say 1000 partitions. Each partition contains a subset of the data, and Spark processes these partitions in parallel across the cluster, making the computation much faster and more efficient.

Checking the Number of Partitions:
The code print("{} partition(s)".format(df_new1.rdd.getNumPartitions())) is used to find out how many partitions are present in the RDD of the DataFrame df_new1.

## Get data types of a DataFrame's columns
"""

print(df_new1.dtypes)

"""## Fill NULL values in specific columns"""

df_A = df_new1.fillna({"population": 0})
df_A.select(col("country"), col("population")).show()

"""## Count the Number if 0 in the column"""

zero_count = df_A.filter(col("population") == 0).count()
print("Number of rows with population = 0:", zero_count)

"""## Fill NULL values with column average"""

df_A = df_A.fillna({"population": df_A.agg(avg("population")).first()[0]})

df_A.show()

"""## Filter based on a specific column value"""

df_new1.where(col("country") == "China").show()

df_new1.filter(col("country") == "China").show()

df_new1.filter(col("fertility_rate") > 1.5).show()

"""## Multiple Columns in Filter"""

df_new1.filter(((col("population") > 50000000) & (col("fertility_rate") > 5))).show()

"""## Filter based on an IN list"""

from pyspark.sql.functions import col

df_new1.where(col("country").isin(["Germany", "Turkey"])).show()

"""## Filter based on a NOT IN list"""

df_zor = df_new1.where(~col("country").isin(["China"]))

df_zor.select(col("fertility_rate"),col("country").isin(["China"])).show()

"""## Get Dataframe rows that match a substring"""

df_new1.where(df_new1.country.contains("Turkey")).show()

"""## Filter a Dataframe based on a custom substring search"""

df_new1.where(col("country").like("T%")).show()

"""## Filter based on a column's length"""

df_new1.where(length(col("country")) < 5).show()

"""## Multiple filter conditions

AND (&) Operator: Rows must meet all conditions to be included.

OR (|) Operator: Rows can meet any of the conditions to be included.
"""

df_new1.filter(((col("population") < 500000000) & (col("fertility_rate") < 2))).show()

df_new1.filter(((col("population") < 500000000) | (col("fertility_rate") < 2))).show()

"""## **Some of the most commonly used operators:**

**Comparison Operators:**

Equality: ==

Not Equal: !=

Greater Than: >

Greater Than or Equal To: >=

Less Than: <

Less Than or Equal To: <=


**Logical Operators:**

AND: &

OR: |

NOT: ~

## Sort DataFrame by a column and Limit
"""

df_new1.orderBy(col("population").desc()).limit(5).show()

"""## Check for Duplicates and Count: Use groupBy and count to identify duplicate values and their counts."""

# Group by the column "population" and count the occurrences
duplicates_df = df_new1.groupBy("country").count()

# Filter to show only the duplicates
duplicates_df = duplicates_df.filter(col("count") > 1)

# Display the duplicates and their counts
duplicates_df.show()

"""## Check for Duplicates in Each Column: Iterate over each column, group by that column, count the occurrences, and filter to show only duplicates."""

from pyspark.sql.functions import col, count

def check_duplicates(df_new1):
    columns = df_new1.columns
    for column in columns:
        duplicates_df1 = df_new1.groupBy(column).count().filter(col("count") > 1).orderBy(col("count").desc())
        print(f"Duplicates in column '{column}':")
        duplicates_df1.show()

# Check for duplicates in each column of df_new1
check_duplicates(df_new1)